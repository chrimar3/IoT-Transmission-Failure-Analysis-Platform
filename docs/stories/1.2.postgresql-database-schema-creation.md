# Story 1.2: PostgreSQL Database Schema Creation

## Status
Draft

## Story
**As a** developer,
**I want** a properly structured PostgreSQL database schema optimized for time-series data,
**so that** the Bangkok dataset (4-6M sensor readings) can be efficiently stored, queried, and scaled for multi-tenant access.

## Acceptance Criteria
1. Schema supports Bangkok dataset structure (134 sensors, 7 floors, 18 months of data)
2. Optimized indexes for time-series queries with sub-500ms response times for common queries
3. Materialized views for daily/hourly aggregations to support dashboard performance
4. Migration scripts for schema updates with rollback capabilities
5. Supports multi-tenant data isolation with Row Level Security (RLS)
6. Database performance testing validates <10 minute bulk import and <500ms API response times
7. Production-ready monitoring and alerting for database health

## Priority & Effort
**Priority**: P0 (Blocking) - Critical dependency for Story 1.3 (Supabase Integration) and all subsequent data access
**Effort**: 3 points
**Epic**: Epic 1 - Core Data Foundation

## Tasks / Subtasks
- [ ] **Task 1: Design Core Time-Series Schema** (AC: 1, 2)
  - [ ] Create `sensor_readings` table with optimized column types and constraints
  - [ ] Implement time-based partitioning strategy for 4-6M records
  - [ ] Add proper TIMESTAMPTZ indexing with BRIN indexes for time-series performance
  - [ ] Create composite indexes for sensor_id + timestamp queries
  - [ ] Add equipment_type and floor_number indexes for filtering

- [ ] **Task 2: Implement Multi-Tenant Data Isolation** (AC: 5)
  - [ ] Create `subscriptions` table with user_id foreign key to auth.users
  - [ ] Implement Row Level Security (RLS) policies on sensor_readings
  - [ ] Create tenant isolation functions for data access control
  - [ ] Add subscription tier-based access control policies

- [ ] **Task 3: Create Performance-Optimized Materialized Views** (AC: 3)
  - [ ] Design `daily_aggregates` materialized view with AVG, MAX, MIN, COUNT
  - [ ] Create `hourly_aggregates` view for real-time dashboard needs
  - [ ] Implement refresh strategy for materialized views (scheduled via cron)
  - [ ] Add indexes on materialized views for fast dashboard queries

- [ ] **Task 4: Database Migration System** (AC: 4)
  - [ ] Create migration scripts in `scripts/setup-database.js`
  - [ ] Implement rollback procedures for all schema changes
  - [ ] Add database version tracking table
  - [ ] Create seed data script for development environment

- [ ] **Task 5: Performance Testing & Validation** (AC: 6)
  - [ ] Create bulk import test with Bangkok dataset subset
  - [ ] Benchmark common API query patterns (<500ms requirement)
  - [ ] Validate materialized view refresh performance
  - [ ] Load test with 4-6M record simulation

- [ ] **Task 6: Production Monitoring Setup** (AC: 7)
  - [ ] Configure Supabase monitoring for query performance
  - [ ] Set up alerts for slow queries (>500ms threshold)
  - [ ] Implement connection pool monitoring
  - [ ] Add database health check endpoints

## Dev Notes

### Database Architecture Context
[Source: docs/architecture/7-database-schema-bangkok-dataset-optimized.md]

The database schema is specifically optimized for the Bangkok CU-BEMS dataset containing:
- 134 unique sensors across 7 floors
- 18 months of data (2018-2019)
- Estimated 4-6 million sensor readings
- Equipment types: HVAC, lighting, electrical systems
- Multi-tenant subscription access (Free, Professional tiers)

### Core Schema Design
[Source: docs/architecture/7-database-schema-bangkok-dataset-optimized.md]

```sql
-- Primary sensor data table optimized for time-series
CREATE TABLE sensor_readings (
    id BIGSERIAL PRIMARY KEY,
    timestamp TIMESTAMPTZ NOT NULL,
    sensor_id VARCHAR(50) NOT NULL,
    floor_number INTEGER NOT NULL,
    equipment_type VARCHAR(100),
    reading_value DECIMAL(10,4),
    unit VARCHAR(20),
    status VARCHAR(20) DEFAULT 'normal'
);

-- Multi-tenant subscription management
CREATE TABLE subscriptions (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    user_id UUID REFERENCES auth.users(id),
    tier VARCHAR(20) NOT NULL DEFAULT 'free',
    stripe_subscription_id VARCHAR(100),
    status VARCHAR(20) DEFAULT 'active',
    created_at TIMESTAMPTZ DEFAULT NOW(),
    expires_at TIMESTAMPTZ
);
```

### Time-Series Optimization Strategy
[Source: docs/architecture/1-bangkok-dataset-integration-pipeline.md]

For handling 4-6M records efficiently:
1. **Partitioning**: Use time-based partitioning (monthly partitions) for sensor_readings
2. **Indexing**: BRIN indexes on timestamp for time-series queries, B-tree for sensor_id lookups
3. **Materialized Views**: Pre-computed aggregations to avoid real-time calculations on large datasets
4. **Import Strategy**: Use PostgreSQL COPY command for bulk data import (pg_copy)

### Multi-Tenant Data Isolation
[Source: docs/architecture/9-security-implementation.md]

Implement Row Level Security (RLS) policies:
- Free tier: Access to sample data only (10% of dataset)
- Professional tier: Full dataset access
- User data isolation via user_id in subscriptions table
- No direct access to sensor_readings without valid subscription

### Technology Stack Integration
[Source: docs/architecture/2-ultra-lean-technology-stack.md]

- **Database**: Supabase PostgreSQL (Free tier: 500MB limit)
- **Connection**: Supabase client with connection pooling
- **Environment**: Development (Docker PostgreSQL), Staging (Supabase subset), Production (Supabase full)

### File Locations and Structure
[Source: docs/architecture/source-tree.md]

```
scripts/
├── setup-database.js       # Database schema setup script
├── process-dataset.js      # Bangkok dataset processing pipeline
└── validate-data.js        # Data integrity validation

src/lib/
├── database.ts             # Supabase client configuration
└── validations.ts          # Zod schemas for database types

src/types/
└── database.ts             # TypeScript database schema types
```

### Production Readiness Requirements

**Performance Benchmarks**:
- Bulk import: Complete Bangkok dataset processing in <10 minutes
- API queries: Response times <500ms for 95th percentile
- Materialized view refresh: <2 minutes for daily aggregates

**Monitoring & Alerting**:
- Query performance monitoring via Supabase dashboard
- Connection pool utilization alerts
- Slow query logging (>500ms threshold)
- Database storage utilization monitoring

**Security Requirements**:
- All database connections use SSL/TLS encryption
- Row Level Security (RLS) policies enforced
- No direct database access for client applications
- Audit logging for data access patterns

**Failure Handling**:
- Database connection retry logic with exponential backoff
- Transaction rollback procedures for failed imports
- Backup and recovery procedures documented
- Migration rollback scripts for all schema changes

### Testing Standards
[Source: docs/architecture/5-testing-framework-setup-installation.md]

**Test Database Strategy**:
- Local: Docker PostgreSQL with sample dataset
- CI: Supabase test project with sanitized data
- Staging: Separate Supabase project with 10% dataset

**Testing Requirements**:
- Unit tests for all database utility functions (100% coverage)
- Integration tests for schema migration scripts
- Performance tests with realistic data volumes
- Data validation tests for import procedures

**Test File Locations**:
```
__tests__/
├── database/
│   ├── schema-migration.test.ts
│   ├── performance-benchmarks.test.ts
│   └── rls-policies.test.ts
└── lib/
    └── database.test.ts
```

**Testing Frameworks**:
- Jest for unit and integration tests
- Supertest for API endpoint testing with database
- Custom performance testing scripts for bulk operations

## Change Log
| Date | Version | Description | Author |
|------|---------|-------------|---------|
| 2025-01-11 | 1.0 | Initial story creation and enhancement following create-next-story workflow | Scrum Master |

## Dev Agent Record
*This section will be populated by the development agent during implementation*

### Agent Model Used
*To be filled by dev agent*

### Debug Log References
*To be filled by dev agent*

### Completion Notes List
*To be filled by dev agent*

### File List
*To be filled by dev agent*

## QA Results
*Results from QA Agent review of the completed story implementation will be recorded here*